{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"# Constants\n\nIMAGE_SIZE = (480, 480)\n\nIMAGES_DIR = \"/media/alperen/Alperen HardDisk/Teknofest 2023/YARISMA 2/PNG VERSIONS/\"\nIDS_DIR = \"/media/alperen/Alperen HardDisk/Teknofest 2023/YARISMA 2/PNG VERSIONS/\"\n\nIMAGE_COUNT = 4\nIMAGE_NAMES = [\"LCC.png\", \"LMLO.png\", \"RCC.png\", \"RMLO.png\"]\n\nFIRST_TASK_NAMES = [\"BI-RADS1-2\", \"BI-RADS0\", \"BI-RADS4-5\"]\nSECOND_TASK_NAMES = [\"A\", \"B\", \"C\", \"D\"]\nTHIRD_TASK_NAMES = [\"SOL ÜST DIŞ\", \"SOL ÜST İÇ\", \"SOL ALT DIŞ\", \"SOL ALT İÇ\", \"SOL MERKEZ\", \"SAĞ ÜST DIŞ\", \"SAĞ ÜST İÇ\", \"SAĞ ALT DIŞ\", \"SAĞ ALT İÇ\", \"SAĞ MERKEZ\", \"NA\"]\n\nBATCH_SIZE = 1\n\nNORM_MEAN = 0.445313\nNORM_STD = 0.269246\n\nimport os\n\nids = os.listdir(IDS_DIR)\n\nimport torch\n\nif torch.cuda.is_available():  \n    DEVICE = 'cuda'\nelse:  \n    DEVICE = 'cpu'","metadata":{"ExecuteTime":{"end_time":"2023-04-28T13:30:58.519634Z","start_time":"2023-04-28T13:30:55.667675Z"},"code_folding":[]},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\nplt.style.use('ggplot')\n\nplt.ioff()\n\ndef show_image(image, show_now = True):\n    if(show_now):\n        plt.figure(figsize = (10, 10))\n        \n    plt.imshow(image, cmap = plt.cm.gray, vmin = 0, vmax = 255)\n        \n    plt.xticks(())\n    plt.yticks(())\n    \n    if(show_now):     \n        plt.show()\n        plt.close()\n        \ndef show_all(images):\n    row_cnt = len(images)\n    col_cnt = len(images[0])\n    \n    plt.figure(figsize = (col_cnt * 10, row_cnt * 10))\n    \n    for row in range(row_cnt):\n        for col in range(col_cnt):\n            plt.subplot(row_cnt, col_cnt, row * col_cnt + (col + 1))\n            show_image(images[row][col], False)\n            \n    plt.show()\n    plt.close()\n    \nimport cv2\n\ndef read_image(path):\n    return cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n\ndef unnormalize(image):\n    image -= image.min()\n    \n    return image * (255 / image.max())","metadata":{"ExecuteTime":{"end_time":"2023-04-28T13:31:12.606588Z","start_time":"2023-04-28T13:31:12.146036Z"}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import albumentations as A\n\nfrom torch.utils.data import Dataset\n\nclass Breast_Cancer_DataSet(Dataset):\n    def __init__(self, id_list):\n        self.images = np.zeros((len(id_list), IMAGE_COUNT) + IMAGE_SIZE, dtype = np.uint8)\n        \n        for i in range(len(id_list)):      \n            if(i % 100 == 0):\n                print(str(i) + \"/\" + str(len(id_list)))\n                \n            for j in range(IMAGE_COUNT):\n                self.images[i][j] = read_image(IMAGES_DIR + id_list[i] + \"/\" + IMAGE_NAMES[j])\n                \n                \n    def __len__(self):\n        return self.images.shape[0]\n    \n    def __getitem__(self, indx):\n            return self.images[indx]\n        \ndatasets = {}\n\ndatasets['teknofest'] = Breast_Cancer_DataSet(ids)\n\nfrom torch.utils.data import DataLoader\n\nloaders = {}\n\nloaders['teknofest'] = DataLoader(datasets['teknofest'], batch_size = BATCH_SIZE, num_workers = 1, pin_memory = True)","metadata":{"ExecuteTime":{"end_time":"2023-04-28T13:35:19.993500Z","start_time":"2023-04-28T13:31:35.050130Z"}},"execution_count":7,"outputs":[{"name":"stdout","output_type":"stream","text":"0/2000\n\n100/2000\n\n200/2000\n\n300/2000\n\n400/2000\n\n500/2000\n\n600/2000\n\n700/2000\n\n800/2000\n\n900/2000\n\n1000/2000\n\n1100/2000\n\n1200/2000\n\n1300/2000\n\n1400/2000\n\n1500/2000\n\n1600/2000\n\n1700/2000\n\n1800/2000\n\n1900/2000\n"}]},{"cell_type":"code","source":"import copy\nimport math\nimport warnings\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union\n\nimport torch\nfrom torch import nn, Tensor\nfrom torchvision.ops import StochasticDepth\n\nfrom torchvision.ops.misc import Conv2dNormActivation, SqueezeExcitation\nfrom torchvision.transforms._presets import ImageClassification, InterpolationMode\nfrom torchvision.utils import _log_api_usage_once\nfrom torchvision.models._api import Weights, WeightsEnum\nfrom torchvision.models._meta import _IMAGENET_CATEGORIES\nfrom torchvision.models._utils import _make_divisible, _ovewrite_named_param, handle_legacy_interface\n\n\n__all__ = [\n    \"EfficientNet\",\n    \"EfficientNet_B0_Weights\",\n    \"EfficientNet_B1_Weights\",\n    \"EfficientNet_B2_Weights\",\n    \"EfficientNet_B3_Weights\",\n    \"EfficientNet_B4_Weights\",\n    \"EfficientNet_B5_Weights\",\n    \"EfficientNet_B6_Weights\",\n    \"EfficientNet_B7_Weights\",\n    \"EfficientNet_V2_S_Weights\",\n    \"EfficientNet_V2_M_Weights\",\n    \"EfficientNet_V2_L_Weights\",\n    \"efficientnet_b0\",\n    \"efficientnet_b1\",\n    \"efficientnet_b2\",\n    \"efficientnet_b3\",\n    \"efficientnet_b4\",\n    \"efficientnet_b5\",\n    \"efficientnet_b6\",\n    \"efficientnet_b7\",\n    \"efficientnet_v2_s\",\n    \"efficientnet_v2_m\",\n    \"efficientnet_v2_l\",\n]\n\n\n@dataclass\nclass _MBConvConfig:\n    expand_ratio: float\n    kernel: int\n    stride: int\n    input_channels: int\n    out_channels: int\n    num_layers: int\n    block: Callable[..., nn.Module]\n\n    @staticmethod\n    def adjust_channels(channels: int, width_mult: float, min_value: Optional[int] = None) -> int:\n        return _make_divisible(channels * width_mult, 8, min_value)\n\n\nclass MBConvConfig(_MBConvConfig):\n    # Stores information listed at Table 1 of the EfficientNet paper & Table 4 of the EfficientNetV2 paper\n    def __init__(\n        self,\n        expand_ratio: float,\n        kernel: int,\n        stride: int,\n        input_channels: int,\n        out_channels: int,\n        num_layers: int,\n        width_mult: float = 1.0,\n        depth_mult: float = 1.0,\n        block: Optional[Callable[..., nn.Module]] = None,\n    ) -> None:\n        input_channels = self.adjust_channels(input_channels, width_mult)\n        out_channels = self.adjust_channels(out_channels, width_mult)\n        num_layers = self.adjust_depth(num_layers, depth_mult)\n        if block is None:\n            block = MBConv\n        super().__init__(expand_ratio, kernel, stride, input_channels, out_channels, num_layers, block)\n\n    @staticmethod\n    def adjust_depth(num_layers: int, depth_mult: float):\n        return int(math.ceil(num_layers * depth_mult))\n\n\nclass FusedMBConvConfig(_MBConvConfig):\n    # Stores information listed at Table 4 of the EfficientNetV2 paper\n    def __init__(\n        self,\n        expand_ratio: float,\n        kernel: int,\n        stride: int,\n        input_channels: int,\n        out_channels: int,\n        num_layers: int,\n        block: Optional[Callable[..., nn.Module]] = None,\n    ) -> None:\n        if block is None:\n            block = FusedMBConv\n        super().__init__(expand_ratio, kernel, stride, input_channels, out_channels, num_layers, block)\n\n\nclass MBConv(nn.Module):\n    def __init__(\n        self,\n        cnf: MBConvConfig,\n        stochastic_depth_prob: float,\n        norm_layer: Callable[..., nn.Module],\n        se_layer: Callable[..., nn.Module] = SqueezeExcitation,\n    ) -> None:\n        super().__init__()\n\n        if not (1 <= cnf.stride <= 2):\n            raise ValueError(\"illegal stride value\")\n\n        self.use_res_connect = cnf.stride == 1 and cnf.input_channels == cnf.out_channels\n\n        layers: List[nn.Module] = []\n        activation_layer = nn.SiLU\n\n        # expand\n        expanded_channels = cnf.adjust_channels(cnf.input_channels, cnf.expand_ratio)\n        if expanded_channels != cnf.input_channels:\n            layers.append(\n                Conv2dNormActivation(\n                    cnf.input_channels,\n                    expanded_channels,\n                    kernel_size=1,\n                    norm_layer=norm_layer,\n                    activation_layer=activation_layer,\n                )\n            )\n\n        # depthwise\n        layers.append(\n            Conv2dNormActivation(\n                expanded_channels,\n                expanded_channels,\n                kernel_size=cnf.kernel,\n                stride=cnf.stride,\n                groups=expanded_channels,\n                norm_layer=norm_layer,\n                activation_layer=activation_layer,\n            )\n        )\n\n        # squeeze and excitation\n        squeeze_channels = max(1, cnf.input_channels // 4)\n        layers.append(se_layer(expanded_channels, squeeze_channels, activation=partial(nn.SiLU, inplace=True)))\n\n        # project\n        layers.append(\n            Conv2dNormActivation(\n                expanded_channels, cnf.out_channels, kernel_size=1, norm_layer=norm_layer, activation_layer=None\n            )\n        )\n\n        self.block = nn.Sequential(*layers)\n        self.stochastic_depth = StochasticDepth(stochastic_depth_prob, \"row\")\n        self.out_channels = cnf.out_channels\n\n    def forward(self, input: Tensor) -> Tensor:\n        result = self.block(input)\n        if self.use_res_connect:\n            result = self.stochastic_depth(result)\n            result += input\n        return result\n\n\nclass FusedMBConv(nn.Module):\n    def __init__(\n        self,\n        cnf: FusedMBConvConfig,\n        stochastic_depth_prob: float,\n        norm_layer: Callable[..., nn.Module],\n    ) -> None:\n        super().__init__()\n\n        if not (1 <= cnf.stride <= 2):\n            raise ValueError(\"illegal stride value\")\n\n        self.use_res_connect = cnf.stride == 1 and cnf.input_channels == cnf.out_channels\n\n        layers: List[nn.Module] = []\n        activation_layer = nn.SiLU\n\n        expanded_channels = cnf.adjust_channels(cnf.input_channels, cnf.expand_ratio)\n        if expanded_channels != cnf.input_channels:\n            # fused expand\n            layers.append(\n                Conv2dNormActivation(\n                    cnf.input_channels,\n                    expanded_channels,\n                    kernel_size=cnf.kernel,\n                    stride=cnf.stride,\n                    norm_layer=norm_layer,\n                    activation_layer=activation_layer,\n                )\n            )\n\n            # project\n            layers.append(\n                Conv2dNormActivation(\n                    expanded_channels, cnf.out_channels, kernel_size=1, norm_layer=norm_layer, activation_layer=None\n                )\n            )\n        else:\n            layers.append(\n                Conv2dNormActivation(\n                    cnf.input_channels,\n                    cnf.out_channels,\n                    kernel_size=cnf.kernel,\n                    stride=cnf.stride,\n                    norm_layer=norm_layer,\n                    activation_layer=activation_layer,\n                )\n            )\n\n        self.block = nn.Sequential(*layers)\n        self.stochastic_depth = StochasticDepth(stochastic_depth_prob, \"row\")\n        self.out_channels = cnf.out_channels\n\n    def forward(self, input: Tensor) -> Tensor:\n        result = self.block(input)\n        if self.use_res_connect:\n            result = self.stochastic_depth(result)\n            result += input\n        return result\n\n\nclass EfficientNet(nn.Module):\n    def __init__(\n        self,\n        inverted_residual_setting: Sequence[Union[MBConvConfig, FusedMBConvConfig]],\n        dropout: float,\n        stochastic_depth_prob: float = 0.2,\n        num_classes: int = 1000,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n        last_channel: Optional[int] = None,\n    ) -> None:\n        \"\"\"\n        EfficientNet V1 and V2 main class\n\n        Args:\n            inverted_residual_setting (Sequence[Union[MBConvConfig, FusedMBConvConfig]]): Network structure\n            dropout (float): The droupout probability\n            stochastic_depth_prob (float): The stochastic depth probability\n            num_classes (int): Number of classes\n            norm_layer (Optional[Callable[..., nn.Module]]): Module specifying the normalization layer to use\n            last_channel (int): The number of channels on the penultimate layer\n        \"\"\"\n        super().__init__()\n        _log_api_usage_once(self)\n\n        if not inverted_residual_setting:\n            raise ValueError(\"The inverted_residual_setting should not be empty\")\n        elif not (\n            isinstance(inverted_residual_setting, Sequence)\n            and all([isinstance(s, _MBConvConfig) for s in inverted_residual_setting])\n        ):\n            raise TypeError(\"The inverted_residual_setting should be List[MBConvConfig]\")\n\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n\n        layers: List[nn.Module] = []\n\n        # building first layer\n        firstconv_output_channels = inverted_residual_setting[0].input_channels\n        layers.append(\n            Conv2dNormActivation(\n                4, firstconv_output_channels, kernel_size=3, stride=2, norm_layer=norm_layer, activation_layer=nn.SiLU\n            )\n        )\n\n        # building inverted residual blocks\n        total_stage_blocks = sum(cnf.num_layers for cnf in inverted_residual_setting)\n        stage_block_id = 0\n        for cnf in inverted_residual_setting:\n            stage: List[nn.Module] = []\n            for _ in range(cnf.num_layers):\n                # copy to avoid modifications. shallow copy is enough\n                block_cnf = copy.copy(cnf)\n\n                # overwrite info if not the first conv in the stage\n                if stage:\n                    block_cnf.input_channels = block_cnf.out_channels\n                    block_cnf.stride = 1\n\n                # adjust stochastic depth probability based on the depth of the stage block\n                sd_prob = stochastic_depth_prob * float(stage_block_id) / total_stage_blocks\n\n                stage.append(block_cnf.block(block_cnf, sd_prob, norm_layer))\n                stage_block_id += 1\n\n            layers.append(nn.Sequential(*stage))\n\n        # building last several layers\n        lastconv_input_channels = inverted_residual_setting[-1].out_channels\n        lastconv_output_channels = last_channel if last_channel is not None else 4 * lastconv_input_channels\n        layers.append(\n            Conv2dNormActivation(\n                lastconv_input_channels,\n                lastconv_output_channels,\n                kernel_size=1,\n                norm_layer=norm_layer,\n                activation_layer=nn.SiLU,\n            )\n        )\n\n        self.features = nn.Sequential(*layers)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=dropout, inplace=True),\n            nn.Linear(lastconv_output_channels, num_classes),\n        )\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                init_range = 1.0 / math.sqrt(m.out_features)\n                nn.init.uniform_(m.weight, -init_range, init_range)\n                nn.init.zeros_(m.bias)\n\n    def _forward_impl(self, x: Tensor) -> Tensor:\n        x = self.features(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n\n        x = self.classifier(x)\n\n        return x\n\n    def forward(self, x: Tensor) -> Tensor:\n        return self._forward_impl(x)\n\n\ndef _efficientnet(\n    inverted_residual_setting: Sequence[Union[MBConvConfig, FusedMBConvConfig]],\n    dropout: float,\n    last_channel: Optional[int],\n    weights: Optional[WeightsEnum],\n    progress: bool,\n    **kwargs: Any,\n) -> EfficientNet:\n#     if weights is not None:\n#         _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n\n    model = EfficientNet(inverted_residual_setting, dropout, last_channel=last_channel, **kwargs)\n\n    if weights is not None:\n        state_dict = weights.get_state_dict(progress=progress)\n        \n        state_dict['features.0.0.weight'] = state_dict['features.0.0.weight'][:, :1].expand(-1, 4, -1, -1)\n        state_dict['classifier.1.weight'] = state_dict['classifier.1.weight'][:CLASS_COUNT, :]\n        state_dict['classifier.1.bias'] = state_dict['classifier.1.bias'][:CLASS_COUNT]\n        \n        model.load_state_dict(state_dict)\n\n    return model\n\n\ndef _efficientnet_conf(\n    arch: str,\n    **kwargs: Any,\n) -> Tuple[Sequence[Union[MBConvConfig, FusedMBConvConfig]], Optional[int]]:\n    inverted_residual_setting: Sequence[Union[MBConvConfig, FusedMBConvConfig]]\n    if arch.startswith(\"efficientnet_b\"):\n        bneck_conf = partial(MBConvConfig, width_mult=kwargs.pop(\"width_mult\"), depth_mult=kwargs.pop(\"depth_mult\"))\n        inverted_residual_setting = [\n            bneck_conf(1, 3, 1, 32, 16, 1),\n            bneck_conf(6, 3, 2, 16, 24, 2),\n            bneck_conf(6, 5, 2, 24, 40, 2),\n            bneck_conf(6, 3, 2, 40, 80, 3),\n            bneck_conf(6, 5, 1, 80, 112, 3),\n            bneck_conf(6, 5, 2, 112, 192, 4),\n            bneck_conf(6, 3, 1, 192, 320, 1),\n        ]\n        last_channel = None\n    elif arch.startswith(\"efficientnet_v2_s\"):\n        inverted_residual_setting = [\n            FusedMBConvConfig(1, 3, 1, 24, 24, 2),\n            FusedMBConvConfig(4, 3, 2, 24, 48, 4),\n            FusedMBConvConfig(4, 3, 2, 48, 64, 4),\n            MBConvConfig(4, 3, 2, 64, 128, 6),\n            MBConvConfig(6, 3, 1, 128, 160, 9),\n            MBConvConfig(6, 3, 2, 160, 256, 15),\n        ]\n        last_channel = 1280\n    elif arch.startswith(\"efficientnet_v2_m\"):\n        inverted_residual_setting = [\n            FusedMBConvConfig(1, 3, 1, 24, 24, 3),\n            FusedMBConvConfig(4, 3, 2, 24, 48, 5),\n            FusedMBConvConfig(4, 3, 2, 48, 80, 5),\n            MBConvConfig(4, 3, 2, 80, 160, 7),\n            MBConvConfig(6, 3, 1, 160, 176, 14),\n            MBConvConfig(6, 3, 2, 176, 304, 18),\n            MBConvConfig(6, 3, 1, 304, 512, 5),\n        ]\n        last_channel = 1280\n    elif arch.startswith(\"efficientnet_v2_l\"):\n        inverted_residual_setting = [\n            FusedMBConvConfig(1, 3, 1, 32, 32, 4),\n            FusedMBConvConfig(4, 3, 2, 32, 64, 7),\n            FusedMBConvConfig(4, 3, 2, 64, 96, 7),\n            MBConvConfig(4, 3, 2, 96, 192, 10),\n            MBConvConfig(6, 3, 1, 192, 224, 19),\n            MBConvConfig(6, 3, 2, 224, 384, 25),\n            MBConvConfig(6, 3, 1, 384, 640, 7),\n        ]\n        last_channel = 1280\n    else:\n        raise ValueError(f\"Unsupported model type {arch}\")\n\n    return inverted_residual_setting, last_channel\n\n\n_COMMON_META: Dict[str, Any] = {\n    \"categories\": _IMAGENET_CATEGORIES,\n}\n\n\n_COMMON_META_V1 = {\n    **_COMMON_META,\n    \"min_size\": (1, 1),\n    \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1\",\n}\n\n\n_COMMON_META_V2 = {\n    **_COMMON_META,\n    \"min_size\": (33, 33),\n    \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v2\",\n}\n\n\nclass EfficientNet_B0_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        # Weights ported from https://github.com/rwightman/pytorch-image-models/\n        url=\"https://download.pytorch.org/models/efficientnet_b0_rwightman-3dd342df.pth\",\n        transforms=partial(\n            ImageClassification, crop_size=224, resize_size=256, interpolation=InterpolationMode.BICUBIC\n        ),\n        meta={\n            **_COMMON_META_V1,\n            \"num_params\": 5288548,\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 77.692,\n                    \"acc@5\": 93.532,\n                }\n            },\n            \"_ops\": 0.386,\n            \"_file_size\": 20.451,\n            \"_docs\": \"\"\"These weights are ported from the original paper.\"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\nclass EfficientNet_B1_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        # Weights ported from https://github.com/rwightman/pytorch-image-models/\n        url=\"https://download.pytorch.org/models/efficientnet_b1_rwightman-533bc792.pth\",\n        transforms=partial(\n            ImageClassification, crop_size=240, resize_size=256, interpolation=InterpolationMode.BICUBIC\n        ),\n        meta={\n            **_COMMON_META_V1,\n            \"num_params\": 7794184,\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 78.642,\n                    \"acc@5\": 94.186,\n                }\n            },\n            \"_ops\": 0.687,\n            \"_file_size\": 30.134,\n            \"_docs\": \"\"\"These weights are ported from the original paper.\"\"\",\n        },\n    )\n    IMAGENET1K_V2 = Weights(\n        url=\"https://download.pytorch.org/models/efficientnet_b1-c27df63c.pth\",\n        transforms=partial(\n            ImageClassification, crop_size=240, resize_size=255, interpolation=InterpolationMode.BILINEAR\n        ),\n        meta={\n            **_COMMON_META_V1,\n            \"num_params\": 7794184,\n            \"recipe\": \"https://github.com/pytorch/vision/issues/3995#new-recipe-with-lr-wd-crop-tuning\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 79.838,\n                    \"acc@5\": 94.934,\n                }\n            },\n            \"_ops\": 0.687,\n            \"_file_size\": 30.136,\n            \"_docs\": \"\"\"\n                These weights improve upon the results of the original paper by using a modified version of TorchVision's\n                `new training recipe\n                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n            \"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V2\n\n\nclass EfficientNet_B2_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        # Weights ported from https://github.com/rwightman/pytorch-image-models/\n        url=\"https://download.pytorch.org/models/efficientnet_b2_rwightman-bcdf34b7.pth\",\n        transforms=partial(\n            ImageClassification, crop_size=288, resize_size=288, interpolation=InterpolationMode.BICUBIC\n        ),\n        meta={\n            **_COMMON_META_V1,\n            \"num_params\": 9109994,\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 80.608,\n                    \"acc@5\": 95.310,\n                }\n            },\n            \"_ops\": 1.088,\n            \"_file_size\": 35.174,\n            \"_docs\": \"\"\"These weights are ported from the original paper.\"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\nclass EfficientNet_B3_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        # Weights ported from https://github.com/rwightman/pytorch-image-models/\n        url=\"https://download.pytorch.org/models/efficientnet_b3_rwightman-cf984f9c.pth\",\n        transforms=partial(\n            ImageClassification, crop_size=300, resize_size=320, interpolation=InterpolationMode.BICUBIC\n        ),\n        meta={\n            **_COMMON_META_V1,\n            \"num_params\": 12233232,\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 82.008,\n                    \"acc@5\": 96.054,\n                }\n            },\n            \"_ops\": 1.827,\n            \"_file_size\": 47.184,\n            \"_docs\": \"\"\"These weights are ported from the original paper.\"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\nclass EfficientNet_B4_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        # Weights ported from https://github.com/rwightman/pytorch-image-models/\n        url=\"https://download.pytorch.org/models/efficientnet_b4_rwightman-7eb33cd5.pth\",\n        transforms=partial(\n            ImageClassification, crop_size=380, resize_size=384, interpolation=InterpolationMode.BICUBIC\n        ),\n        meta={\n            **_COMMON_META_V1,\n            \"num_params\": 19341616,\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 83.384,\n                    \"acc@5\": 96.594,\n                }\n            },\n            \"_ops\": 4.394,\n            \"_file_size\": 74.489,\n            \"_docs\": \"\"\"These weights are ported from the original paper.\"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\nclass EfficientNet_B5_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        # Weights ported from https://github.com/lukemelas/EfficientNet-PyTorch/\n        url=\"https://download.pytorch.org/models/efficientnet_b5_lukemelas-b6417697.pth\",\n        transforms=partial(\n            ImageClassification, crop_size=456, resize_size=456, interpolation=InterpolationMode.BICUBIC\n        ),\n        meta={\n            **_COMMON_META_V1,\n            \"num_params\": 30389784,\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 83.444,\n                    \"acc@5\": 96.628,\n                }\n            },\n            \"_ops\": 10.266,\n            \"_file_size\": 116.864,\n            \"_docs\": \"\"\"These weights are ported from the original paper.\"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\nclass EfficientNet_B6_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        # Weights ported from https://github.com/lukemelas/EfficientNet-PyTorch/\n        url=\"https://download.pytorch.org/models/efficientnet_b6_lukemelas-c76e70fd.pth\",\n        transforms=partial(\n            ImageClassification, crop_size=528, resize_size=528, interpolation=InterpolationMode.BICUBIC\n        ),\n        meta={\n            **_COMMON_META_V1,\n            \"num_params\": 43040704,\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 84.008,\n                    \"acc@5\": 96.916,\n                }\n            },\n            \"_ops\": 19.068,\n            \"_file_size\": 165.362,\n            \"_docs\": \"\"\"These weights are ported from the original paper.\"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\nclass EfficientNet_B7_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        # Weights ported from https://github.com/lukemelas/EfficientNet-PyTorch/\n        url=\"https://download.pytorch.org/models/efficientnet_b7_lukemelas-dcc49843.pth\",\n        transforms=partial(\n            ImageClassification, crop_size=600, resize_size=600, interpolation=InterpolationMode.BICUBIC\n        ),\n        meta={\n            **_COMMON_META_V1,\n            \"num_params\": 66347960,\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 84.122,\n                    \"acc@5\": 96.908,\n                }\n            },\n            \"_ops\": 37.746,\n            \"_file_size\": 254.675,\n            \"_docs\": \"\"\"These weights are ported from the original paper.\"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\nclass EfficientNet_V2_S_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        url=\"https://download.pytorch.org/models/efficientnet_v2_s-dd5fe13b.pth\",\n        transforms=partial(\n            ImageClassification,\n            crop_size=384,\n            resize_size=384,\n            interpolation=InterpolationMode.BILINEAR,\n        ),\n        meta={\n            **_COMMON_META_V2,\n            \"num_params\": 21458488,\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 84.228,\n                    \"acc@5\": 96.878,\n                }\n            },\n            \"_ops\": 8.366,\n            \"_file_size\": 82.704,\n            \"_docs\": \"\"\"\n                These weights improve upon the results of the original paper by using a modified version of TorchVision's\n                `new training recipe\n                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n            \"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\nclass EfficientNet_V2_M_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        url=\"https://download.pytorch.org/models/efficientnet_v2_m-dc08266a.pth\",\n        transforms=partial(\n            ImageClassification,\n            crop_size=480,\n            resize_size=480,\n            interpolation=InterpolationMode.BILINEAR,\n        ),\n        meta={\n            **_COMMON_META_V2,\n            \"num_params\": 54139356,\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 85.112,\n                    \"acc@5\": 97.156,\n                }\n            },\n            \"_ops\": 24.582,\n            \"_file_size\": 208.01,\n            \"_docs\": \"\"\"\n                These weights improve upon the results of the original paper by using a modified version of TorchVision's\n                `new training recipe\n                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n            \"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\nclass EfficientNet_V2_L_Weights(WeightsEnum):\n    # Weights ported from https://github.com/google/automl/tree/master/efficientnetv2\n    IMAGENET1K_V1 = Weights(\n        url=\"https://download.pytorch.org/models/efficientnet_v2_l-59c71312.pth\",\n        transforms=partial(\n            ImageClassification,\n            crop_size=480,\n            resize_size=480,\n            interpolation=InterpolationMode.BICUBIC,\n            mean=(0.5, 0.5, 0.5),\n            std=(0.5, 0.5, 0.5),\n        ),\n        meta={\n            **_COMMON_META_V2,\n            \"num_params\": 118515272,\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 85.808,\n                    \"acc@5\": 97.788,\n                }\n            },\n            \"_ops\": 56.08,\n            \"_file_size\": 454.573,\n            \"_docs\": \"\"\"These weights are ported from the original paper.\"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\n@handle_legacy_interface(weights=(\"pretrained\", EfficientNet_B0_Weights.IMAGENET1K_V1))\ndef efficientnet_b0(\n    *, weights: Optional[EfficientNet_B0_Weights] = None, progress: bool = True, **kwargs: Any\n) -> EfficientNet:\n    \"\"\"EfficientNet B0 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional\n    Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper.\n\n    Args:\n        weights (:class:`~torchvision.models.EfficientNet_B0_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.EfficientNet_B0_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.EfficientNet_B0_Weights\n        :members:\n    \"\"\"\n    weights = EfficientNet_B0_Weights.verify(weights)\n\n    inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_b0\", width_mult=1.0, depth_mult=1.0)\n    return _efficientnet(\n        inverted_residual_setting, kwargs.pop(\"dropout\", 0.2), last_channel, weights, progress, **kwargs\n    )\n\n\n@handle_legacy_interface(weights=(\"pretrained\", EfficientNet_B1_Weights.IMAGENET1K_V1))\ndef efficientnet_b1(\n    *, weights: Optional[EfficientNet_B1_Weights] = None, progress: bool = True, **kwargs: Any\n) -> EfficientNet:\n    \"\"\"EfficientNet B1 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional\n    Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper.\n\n    Args:\n        weights (:class:`~torchvision.models.EfficientNet_B1_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.EfficientNet_B1_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.EfficientNet_B1_Weights\n        :members:\n    \"\"\"\n    weights = EfficientNet_B1_Weights.verify(weights)\n\n    inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_b1\", width_mult=1.0, depth_mult=1.1)\n    return _efficientnet(\n        inverted_residual_setting, kwargs.pop(\"dropout\", 0.2), last_channel, weights, progress, **kwargs\n    )\n\n\n@handle_legacy_interface(weights=(\"pretrained\", EfficientNet_B2_Weights.IMAGENET1K_V1))\ndef efficientnet_b2(\n    *, weights: Optional[EfficientNet_B2_Weights] = None, progress: bool = True, **kwargs: Any\n) -> EfficientNet:\n    \"\"\"EfficientNet B2 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional\n    Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper.\n\n    Args:\n        weights (:class:`~torchvision.models.EfficientNet_B2_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.EfficientNet_B2_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.EfficientNet_B2_Weights\n        :members:\n    \"\"\"\n    weights = EfficientNet_B2_Weights.verify(weights)\n\n    inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_b2\", width_mult=1.1, depth_mult=1.2)\n    return _efficientnet(\n        inverted_residual_setting, kwargs.pop(\"dropout\", 0.3), last_channel, weights, progress, **kwargs\n    )\n\n\n@handle_legacy_interface(weights=(\"pretrained\", EfficientNet_B3_Weights.IMAGENET1K_V1))\ndef efficientnet_b3(\n    *, weights: Optional[EfficientNet_B3_Weights] = None, progress: bool = True, **kwargs: Any\n) -> EfficientNet:\n    \"\"\"EfficientNet B3 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional\n    Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper.\n\n    Args:\n        weights (:class:`~torchvision.models.EfficientNet_B3_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.EfficientNet_B3_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.EfficientNet_B3_Weights\n        :members:\n    \"\"\"\n    weights = EfficientNet_B3_Weights.verify(weights)\n\n    inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_b3\", width_mult=1.2, depth_mult=1.4)\n    return _efficientnet(\n        inverted_residual_setting,\n        kwargs.pop(\"dropout\", 0.3),\n        last_channel,\n        weights,\n        progress,\n        **kwargs,\n    )\n\n\n@handle_legacy_interface(weights=(\"pretrained\", EfficientNet_B4_Weights.IMAGENET1K_V1))\ndef efficientnet_b4(\n    *, weights: Optional[EfficientNet_B4_Weights] = None, progress: bool = True, **kwargs: Any\n) -> EfficientNet:\n    \"\"\"EfficientNet B4 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional\n    Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper.\n\n    Args:\n        weights (:class:`~torchvision.models.EfficientNet_B4_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.EfficientNet_B4_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.EfficientNet_B4_Weights\n        :members:\n    \"\"\"\n    weights = EfficientNet_B4_Weights.verify(weights)\n\n    inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_b4\", width_mult=1.4, depth_mult=1.8)\n    return _efficientnet(\n        inverted_residual_setting,\n        kwargs.pop(\"dropout\", 0.4),\n        last_channel,\n        weights,\n        progress,\n        **kwargs,\n    )\n\n\n\n@handle_legacy_interface(weights=(\"pretrained\", EfficientNet_B5_Weights.IMAGENET1K_V1))\ndef efficientnet_b5(\n    *, weights: Optional[EfficientNet_B5_Weights] = None, progress: bool = True, **kwargs: Any\n) -> EfficientNet:\n    \"\"\"EfficientNet B5 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional\n    Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper.\n\n    Args:\n        weights (:class:`~torchvision.models.EfficientNet_B5_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.EfficientNet_B5_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.EfficientNet_B5_Weights\n        :members:\n    \"\"\"\n    weights = EfficientNet_B5_Weights.verify(weights)\n\n    inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_b5\", width_mult=1.6, depth_mult=2.2)\n    return _efficientnet(\n        inverted_residual_setting,\n        kwargs.pop(\"dropout\", 0.4),\n        last_channel,\n        weights,\n        progress,\n        norm_layer=partial(nn.BatchNorm2d, eps=0.001, momentum=0.01),\n        **kwargs,\n    )\n\n\n\n@handle_legacy_interface(weights=(\"pretrained\", EfficientNet_B6_Weights.IMAGENET1K_V1))\ndef efficientnet_b6(\n    *, weights: Optional[EfficientNet_B6_Weights] = None, progress: bool = True, **kwargs: Any\n) -> EfficientNet:\n    \"\"\"EfficientNet B6 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional\n    Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper.\n\n    Args:\n        weights (:class:`~torchvision.models.EfficientNet_B6_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.EfficientNet_B6_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.EfficientNet_B6_Weights\n        :members:\n    \"\"\"\n    weights = EfficientNet_B6_Weights.verify(weights)\n\n    inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_b6\", width_mult=1.8, depth_mult=2.6)\n    return _efficientnet(\n        inverted_residual_setting,\n        kwargs.pop(\"dropout\", 0.5),\n        last_channel,\n        weights,\n        progress,\n        norm_layer=partial(nn.BatchNorm2d, eps=0.001, momentum=0.01),\n        **kwargs,\n    )\n\n\n\n@handle_legacy_interface(weights=(\"pretrained\", EfficientNet_B7_Weights.IMAGENET1K_V1))\ndef efficientnet_b7(\n    *, weights: Optional[EfficientNet_B7_Weights] = None, progress: bool = True, **kwargs: Any\n) -> EfficientNet:\n    \"\"\"EfficientNet B7 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional\n    Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper.\n\n    Args:\n        weights (:class:`~torchvision.models.EfficientNet_B7_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.EfficientNet_B7_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.EfficientNet_B7_Weights\n        :members:\n    \"\"\"\n    weights = EfficientNet_B7_Weights.verify(weights)\n\n    inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_b7\", width_mult=2.0, depth_mult=3.1)\n    return _efficientnet(\n        inverted_residual_setting,\n        kwargs.pop(\"dropout\", 0.5),\n        last_channel,\n        weights,\n        progress,\n        norm_layer=partial(nn.BatchNorm2d, eps=0.001, momentum=0.01),\n        **kwargs,\n    )\n\n\n\n@handle_legacy_interface(weights=(\"pretrained\", EfficientNet_V2_S_Weights.IMAGENET1K_V1))\ndef efficientnet_v2_s(\n    *, weights: Optional[EfficientNet_V2_S_Weights] = None, progress: bool = True, **kwargs: Any\n) -> EfficientNet:\n    \"\"\"\n    Constructs an EfficientNetV2-S architecture from\n    `EfficientNetV2: Smaller Models and Faster Training <https://arxiv.org/abs/2104.00298>`_.\n\n    Args:\n        weights (:class:`~torchvision.models.EfficientNet_V2_S_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.EfficientNet_V2_S_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.EfficientNet_V2_S_Weights\n        :members:\n    \"\"\"\n    weights = EfficientNet_V2_S_Weights.verify(weights)\n\n    inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_v2_s\")\n    return _efficientnet(\n        inverted_residual_setting,\n        kwargs.pop(\"dropout\", 0.2),\n        last_channel,\n        weights,\n        progress,\n        norm_layer=partial(nn.BatchNorm2d, eps=1e-03),\n        **kwargs,\n    )\n\n\n\n@handle_legacy_interface(weights=(\"pretrained\", EfficientNet_V2_M_Weights.IMAGENET1K_V1))\ndef efficientnet_v2_m(\n    *, weights: Optional[EfficientNet_V2_M_Weights] = None, progress: bool = True, **kwargs: Any\n) -> EfficientNet:\n    \"\"\"\n    Constructs an EfficientNetV2-M architecture from\n    `EfficientNetV2: Smaller Models and Faster Training <https://arxiv.org/abs/2104.00298>`_.\n\n    Args:\n        weights (:class:`~torchvision.models.EfficientNet_V2_M_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.EfficientNet_V2_M_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.EfficientNet_V2_M_Weights\n        :members:\n    \"\"\"\n    weights = EfficientNet_V2_M_Weights.verify(weights)\n\n    inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_v2_m\")\n    return _efficientnet(\n        inverted_residual_setting,\n        kwargs.pop(\"dropout\", 0.3),\n        last_channel,\n        weights,\n        progress,\n        norm_layer=partial(nn.BatchNorm2d, eps=1e-03),\n        **kwargs,\n    )\n\n\n\n@handle_legacy_interface(weights=(\"pretrained\", EfficientNet_V2_L_Weights.IMAGENET1K_V1))\ndef efficientnet_v2_l(\n    *, weights: Optional[EfficientNet_V2_L_Weights] = None, progress: bool = True, **kwargs: Any\n) -> EfficientNet:\n    \"\"\"\n    Constructs an EfficientNetV2-L architecture from\n    `EfficientNetV2: Smaller Models and Faster Training <https://arxiv.org/abs/2104.00298>`_.\n\n    Args:\n        weights (:class:`~torchvision.models.EfficientNet_V2_L_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.EfficientNet_V2_L_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.EfficientNet_V2_L_Weights\n        :members:\n    \"\"\"\n    weights = EfficientNet_V2_L_Weights.verify(weights)\n\n    inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_v2_l\")\n    return _efficientnet(\n        inverted_residual_setting,\n        kwargs.pop(\"dropout\", 0.4),\n        last_channel,\n        weights,\n        progress,\n        norm_layer=partial(nn.BatchNorm2d, eps=1e-03),\n        **kwargs,\n    )","metadata":{"ExecuteTime":{"end_time":"2023-04-28T13:35:22.165147Z","start_time":"2023-04-28T13:35:21.721688Z"},"code_folding":[19,47,61,87,104,171,232,346,353,371,374,426,431,438,445,469,517,541,565,589,613,637,638,661,691,721,751,753,781,783,811,813,841,843,876,878,912,914,949,951,986,988,1023,1025,1061,1063,1099,1101]},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n\ntorch.cuda.empty_cache()\n\ntorch.backends.cudnn.benchmark = True\ntorch.autograd.set_detect_anomaly(False)\ntorch.autograd.profiler.emit_nvtx(False)\ntorch.autograd.profiler.profile(False)\n\nfirst_task_dirs = [\n    \"/media/alperen/Alperen HardDisk/Teknofest 2023/First Task Saved Models/warm restart, t0 = 20, faster augment _ 0.693777.pth\",\n    \"/media/alperen/Alperen HardDisk/Teknofest 2023/First Task Saved Models/efficientv2s, lr = 1e-3, batch = 64 _ 0.681069.pth\",\n    \"/media/alperen/Alperen HardDisk/Teknofest 2023/First Task Saved Models/efficientv2s, lr = 1e-4, batch = 64 _ 0.678628.pth\",\n    \"/media/alperen/Alperen HardDisk/Teknofest 2023/First Task Saved Models/crop, rotate, downscale _ 0.673207.pth\",\n    \"/media/alperen/Alperen HardDisk/Teknofest 2023/First Task Saved Models/warm restart, t0 = 10, faster augment _ 0.664332.pth\"\n]\n\nfirst_task_class3_dirs = [\n    \"/media/alperen/Alperen HardDisk/Teknofest 2023/First Task Saved Models/class 3 efficientnetb4 lr 1e-4 _ 0.818301.pth\",\n    \"/media/alperen/Alperen HardDisk/Teknofest 2023/First Task Saved Models/class 3 efficientnet_b4 _ 0.804097.pth\",\n    \"/media/alperen/Alperen HardDisk/Teknofest 2023/First Task Saved Models/class 3 efficientnetb4 lr 1e-4 timm _ 0.795396.pth\",\n    \"/media/alperen/Alperen HardDisk/Teknofest 2023/First Task Saved Models/class 3 efficientnetv2_s _ 0.780423.pth\",\n]\n\nsecond_task_dirs = [\n    \"/media/alperen/Alperen HardDisk/Teknofest 2023/First Task Saved Models/task 2 efficientnetv2-s _ 0.766753.pth\",\n    \"/media/alperen/Alperen HardDisk/Teknofest 2023/First Task Saved Models/task 2 efficientnetv2-m _ 0.761323.pth\",\n    \"/media/alperen/Alperen HardDisk/Teknofest 2023/First Task Saved Models/task 2 efficientnetb4 lr 1e-4 _ 0.754723.pth\"\n]\n\nthird_task_dirs = [\n    \"/media/alperen/Alperen HardDisk/Teknofest 2023/First Task Saved Models/task 3 efficientnetv2s _ 0.719799.pth\",\n    \"/media/alperen/Alperen HardDisk/Teknofest 2023/First Task Saved Models/task 3 efficientv2s, lr 1e-4 _ 0.708054.pth\",\n    \"/media/alperen/Alperen HardDisk/Teknofest 2023/First Task Saved Models/task 3 efficientnetv2m _ 0.701342.pth\",\n    \"/media/alperen/Alperen HardDisk/Teknofest 2023/First Task Saved Models/task 3 efficientnetb4 _ 0.687919.pth\"\n]\n\nfirst_task_models = []\n\nfor model_dir in first_task_dirs:\n    first_task_models.append(torch.load(model_dir, map_location = DEVICE)['model'].module)\n    \nfirst_task_class3_models = []\n\nfor model_dir in first_task_class3_dirs:\n    first_task_class3_models.append(torch.load(model_dir, map_location = DEVICE)['model'].module)\n\nsecond_task_models = []\n\nfor model_dir in second_task_dirs:\n    second_task_models.append(torch.load(model_dir, map_location = DEVICE)['model'].module)\n    \nthird_task_models = []\n\nfor model_dir in third_task_dirs:\n    third_task_models.append(torch.load(model_dir, map_location = DEVICE)['model'].module)\n    \nfor model in first_task_models:\n    model.eval()\n    \nfor model in first_task_class3_models:\n    model.eval()\n    \nfor model in second_task_models:\n    model.eval()\n    \nfor model in third_task_models:\n    model.eval()","metadata":{"ExecuteTime":{"end_time":"2023-04-28T13:36:45.750680Z","start_time":"2023-04-28T13:35:24.141778Z"}},"execution_count":9,"outputs":[{"name":"stderr","output_type":"stream","text":"/home/alperen/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n\n  from .autonotebook import tqdm as notebook_tqdm\n"}]},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef normalize(images):\n    return ((images / 255 - NORM_MEAN) / NORM_STD)\n\ndef predict_multiclass(model, image):\n    with torch.no_grad():\n        return torch.nn.Softmax(dim = -1)(model(image))[0]\n\ndef predict_binary(model, image):\n    with torch.no_grad():\n        return torch.nn.Sigmoid()(model(image))[0]\n\ndef predict_first_task(image):\n    predictions = predict_multiclass(first_task_models[0], image)\n    \n    for model in first_task_models:\n        predictions += predict_multiclass(model, image)\n        \n    predictions_class_3 = predict_binary(first_task_class3_models[0], image)\n    \n    for model in first_task_class3_models:\n        predictions_class_3 += predict_binary(model, image)\n    \n    if(predictions_class_3 > 0.5 and predictions.max() < 0.45):\n        return 2\n    else:\n        return predictions.argmax()\n    \ndef predict_second_task(image):\n    predictions = predict_multiclass(second_task_models[0], image)\n    \n    for model in second_task_models:\n        predictions += predict_multiclass(model, image)\n        \n    return predictions.argmax()\n\ndef predict_third_task(image):\n    predictions = predict_multiclass(third_task_models[0], image)\n    \n    for model in third_task_models:\n        predictions += predict_multiclass(model, image)\n        \n    return predictions.argmax()","metadata":{"ExecuteTime":{"end_time":"2023-04-28T13:48:48.606830Z","start_time":"2023-04-28T13:48:48.600615Z"}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\noutputs = pd.DataFrame(columns = [\"HASTANO\", \"BIRADS KATEGORİSİ\", \"MEME KOMPOZİSYONU\", \"KADRAN BİLGİSİ (SAĞ)\", \"KADRAN BİLGİSİ (SOL)\"])\n\noutputs = outputs.astype(\"string\")\noutputs = outputs.set_index(\"HASTANO\")\n\nfor i, image in enumerate(loaders['teknofest']):\n    if(i % 100 == 0):\n        print(i)\n        \n    image = image.to(DEVICE, non_blocking = True)\n    image = normalize(image)\n    \n    first_answer = FIRST_TASK_NAMES[predict_first_task(image)]\n    second_answer = SECOND_TASK_NAMES[predict_second_task(image)]\n    third_answer = THIRD_TASK_NAMES[predict_third_task(image)]\n    \n    if(first_answer == \"BI-RADS1-2\"):\n        output = [first_answer, second_answer, \"\", \"\"]\n    else:\n        output = [first_answer, second_answer, '[\\\"' + third_answer[4:] + '\\\"]' if third_answer[:3] == \"SAĞ\" else \"\", '[\\\"' + third_answer[4:] + '\\\"]' if third_answer[:3] == \"SOL\" else \"\"]\n    \n    outputs.loc[ids[i]] = output","metadata":{"ExecuteTime":{"end_time":"2023-04-28T13:49:53.160009Z","start_time":"2023-04-28T13:49:01.541135Z"},"scrolled":true},"execution_count":17,"outputs":[{"name":"stdout","output_type":"stream","text":"1100\n\n1200\n"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(DEVICE, non_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m image \u001b[38;5;241m=\u001b[39m normalize(image)\n\u001b[0;32m---> 18\u001b[0m first_answer \u001b[38;5;241m=\u001b[39m FIRST_TASK_NAMES[\u001b[43mpredict_first_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     19\u001b[0m second_answer \u001b[38;5;241m=\u001b[39m SECOND_TASK_NAMES[predict_second_task(image)]\n\u001b[1;32m     20\u001b[0m third_answer \u001b[38;5;241m=\u001b[39m THIRD_TASK_NAMES[predict_third_task(image)]\n","Cell \u001b[0;32mIn[16], line 25\u001b[0m, in \u001b[0;36mpredict_first_task\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     20\u001b[0m     predictions_class_3 \u001b[38;5;241m=\u001b[39m predict_binary(first_task_class3_models[\u001b[38;5;241m0\u001b[39m], image)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#     for model in first_task_class3_models[1:3]:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#         predictions_class_3 += predict_binary(model, image)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(predictions_class_3 \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.45\u001b[39m):\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":"outputs.to_csv(\"teknofest_outputs.txt\")","metadata":{"ExecuteTime":{"end_time":"2023-04-28T13:49:54.723849Z","start_time":"2023-04-28T13:49:54.682659Z"}},"execution_count":18,"outputs":[]}]}
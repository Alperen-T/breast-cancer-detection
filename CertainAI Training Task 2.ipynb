{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install --force-reinstall torch==1.12.1 torchvision==0.13.1","metadata":{"_uuid":"d0b5cea2-5804-4825-a394-56c488ef0d69","_cell_guid":"9bf535d1-f48f-4da0-b2c7-00e4b6ebfea5","execution":{"iopub.status.busy":"2023-04-16T00:29:57.208557Z","iopub.execute_input":"2023-04-16T00:29:57.209262Z","iopub.status.idle":"2023-04-16T00:31:35.054020Z","shell.execute_reply.started":"2023-04-16T00:29:57.209218Z","shell.execute_reply":"2023-04-16T00:31:35.052773Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constants\nPROJECT_NAME = \"CertainAI Breast | 2\"\n\nIMAGE_SIZE = (480, 480)\n\nIMAGES_DIR = \"/kaggle/input/breast-cancer-dataset/images/\"\nLABELS_DIR = \"/kaggle/input/breast-cancer-dataset/labels.xlsx\"\nTRAIN_IDS = \"/kaggle/input/breast-cancer-dataset/train.txt\"\nVALID_IDS = \"/kaggle/input/breast-cancer-dataset/valid.txt\"\n\nIMAGE_COUNT = 4\nIMAGE_NAMES = [\"LCC.png\", \"LMLO.png\", \"RCC.png\", \"RMLO.png\"]\n\nCLASS_COUNT = 4\nCLASS_NAMES = [\"A\", \"B\", \"C\", \"D\"]\nLABEL_COLS = [1, 1, 1, 1]\n\nBATCH_SIZE = 24\nLEARNING_RATE = 1e-3\n\nEPOCH_COUNT = 1000\n\nNORM_MEAN = 0.445313\nNORM_STD = 0.269246\n\nimport os\n\ntrain_ids = [patient[:-1] if patient[-1] == '\\n' else patient for patient in open(TRAIN_IDS, 'r').readlines()]\nvalid_ids = [patient[:-1] if patient[-1] == '\\n' else patient for patient in open(VALID_IDS, 'r').readlines()]\n\nimport torch\n\nif torch.cuda.is_available():  \n    DEVICE = 'cuda'\nelse:  \n    DEVICE = 'cpu'","metadata":{"_uuid":"53a9918d-ec11-412d-8fdf-efe1de2cc24a","_cell_guid":"32ab965f-131b-4623-9e49-106ba02c6076","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-21T22:16:06.072287Z","iopub.execute_input":"2023-04-21T22:16:06.072655Z","iopub.status.idle":"2023-04-21T22:16:08.433309Z","shell.execute_reply.started":"2023-04-21T22:16:06.072622Z","shell.execute_reply":"2023-04-21T22:16:08.432055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\nplt.style.use('ggplot')\n\nplt.ioff()\n\ndef show_image(image, show_now = True):\n    if(show_now):\n        plt.figure(figsize = (10, 10))\n        \n    plt.imshow(image, cmap = plt.cm.gray, vmin = 0, vmax = 255)\n        \n    plt.xticks(())\n    plt.yticks(())\n    \n    if(show_now):     \n        plt.show()\n        plt.close()\n        \ndef show_all(images):\n    row_cnt = len(images)\n    col_cnt = len(images[0])\n    \n    plt.figure(figsize = (col_cnt * 10, row_cnt * 10))\n    \n    for row in range(row_cnt):\n        for col in range(col_cnt):\n            plt.subplot(row_cnt, col_cnt, row * col_cnt + (col + 1))\n            show_image(images[row][col], False)\n            \n    plt.show()\n    plt.close()\n    \nimport cv2\n\ndef read_image(path):\n    return cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n\ndef unnormalize(image):\n    image -= image.min()\n    \n    return image * (255 / image.max())","metadata":{"_uuid":"091a1c58-af7b-49da-b20f-bda32899f997","_cell_guid":"b3db553b-d41d-4e4d-945a-d949140f2fcf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-21T22:16:23.898036Z","iopub.execute_input":"2023-04-21T22:16:23.898607Z","iopub.status.idle":"2023-04-21T22:16:24.238908Z","shell.execute_reply.started":"2023-04-21T22:16:23.898565Z","shell.execute_reply":"2023-04-21T22:16:24.237639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations as A\n\ntransform = A.Compose([\n    A.RandomSizedCrop((IMAGE_SIZE[0] - IMAGE_SIZE[0] // 8, IMAGE_SIZE[0]), height = IMAGE_SIZE[0], width = IMAGE_SIZE[1], interpolation = 2, p = 0.8), # 2\n    \n#     A.OneOf([\n#         A.RandomToneCurve(scale=0.2, p=0.4), # 5\n#         A.RandomBrightnessContrast(brightness_limit=(-0.1, 0.2), contrast_limit=(-0.4, 0.5), brightness_by_max=True, always_apply=False, p=0.6) # 2\n#     ], p=0.9),\n    \n    A.OneOf(\n    [\n        A.ShiftScaleRotate(shift_limit=None, scale_limit=[-0.15, 0.15], rotate_limit=[-20, 20], interpolation=2,\n                           border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=None, shift_limit_x=[-0.1, 0.1],\n                           shift_limit_y=[-0.2, 0.2], rotate_method='largest_box', p=0.7), # 10\n        A.GridDistortion(num_steps=5, distort_limit=0.3, interpolation=2, border_mode=cv2.BORDER_CONSTANT,\n                         value=0, mask_value=None, normalized=True, p=0.3), # 15\n    ], p=0.9),\n    \n    A.CoarseDropout(max_holes=6, max_height=0.15, max_width=0.25, min_holes=1, min_height=0.05, min_width=0.1,\n                    fill_value=0, mask_fill_value=None, p=0.5), # 0\n])\n\n\nfrom torch.utils.data import Dataset\n\nclass Breast_Cancer_DataSet(Dataset):\n    def __init__(self, id_list, labels, transform = None):\n        self.transform = transform\n        \n        self.images = np.zeros((len(id_list), IMAGE_COUNT) + IMAGE_SIZE, dtype = np.uint8)\n        self.labels = np.zeros(len(id_list), dtype = np.int64)\n        \n        for i in range(len(id_list)):      \n            if(i % 100 == 0):\n                print(str(i) + \"/\" + str(len(id_list)))\n                \n            for j in range(IMAGE_COUNT):\n                self.images[i][j] = read_image(IMAGES_DIR + id_list[i] + \"/\" + IMAGE_NAMES[j])\n                \n            patient_labels = labels.loc[id_list[i]]\n                \n            for j in range(0, CLASS_COUNT):\n                if(CLASS_NAMES[j] in patient_labels.iloc[LABEL_COLS[j]]):\n                    self.labels[i] = j\n                    break\n                \n    def __len__(self):\n        return self.images.shape[0]\n    \n    def __getitem__(self, indx):\n        if(self.transform):\n            transformed_images = np.zeros((IMAGE_COUNT,) + IMAGE_SIZE, dtype = np.uint8)\n            \n            for i in range(IMAGE_COUNT):\n                transformed_images[i] = self.transform(image = self.images[indx][i])['image']\n                \n            return (transformed_images, self.labels[indx])\n        else:\n            return (self.images[indx], self.labels[indx])\n            \nimport pandas as pd\n\nlabels_df = pd.read_excel(LABELS_DIR)\nlabels_df = labels_df.astype(\"string\")\nlabels_df = labels_df.set_index(\"HASTANO\")\n        \ndatasets = {}\n\ndatasets['train'] = Breast_Cancer_DataSet(train_ids, labels_df, transform)\ndatasets['valid'] = Breast_Cancer_DataSet(valid_ids, labels_df)\n\nweights = [7.407407407407407, 2.849002849002849, 2.865329512893983, 6.0606060606060606]\n\nall_weights = [0] * len(datasets['train'])\n\nfor i, label in enumerate(datasets['train'].labels):\n    all_weights[i] = weights[label]\n\nfrom torch.utils.data import DataLoader\n\nloaders = {}\nloaders['train'] = DataLoader(datasets['train'], batch_size = BATCH_SIZE, num_workers = 2, pin_memory = True, sampler = torch.utils.data.WeightedRandomSampler(all_weights, len(datasets['train'])))\nloaders['valid'] = DataLoader(datasets['valid'], batch_size = BATCH_SIZE, num_workers = 2, pin_memory = True, shuffle = True)","metadata":{"_uuid":"47287855-7ede-4dba-88a8-1083d6332fd0","_cell_guid":"09ef309f-f175-4da6-a9b0-c199ebd8b2e1","execution":{"iopub.status.busy":"2023-04-21T22:20:04.544158Z","iopub.execute_input":"2023-04-21T22:20:04.545092Z","iopub.status.idle":"2023-04-21T22:20:31.147951Z","shell.execute_reply.started":"2023-04-21T22:20:04.545050Z","shell.execute_reply":"2023-04-21T22:20:31.146606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchmetrics\n\nmetrics = torchmetrics.MetricCollection([\n    torchmetrics.Precision(task = 'multiclass', num_classes = CLASS_COUNT, average = 'macro', validate_args = False),\n    torchmetrics.Recall(task = 'multiclass', num_classes = CLASS_COUNT, average = 'macro', validate_args = False),\n    torchmetrics.F1Score(task = 'multiclass', num_classes = CLASS_COUNT, average = 'macro', validate_args = False),\n    torchmetrics.ConfusionMatrix(task = 'multiclass', num_classes = CLASS_COUNT, average = 'macro', validate_args = False)\n])\n\nmetrics.to(DEVICE)","metadata":{"_uuid":"3315e9a9-19ba-47bf-8e7b-0cbd280f44be","_cell_guid":"777a7328-97b2-4d6f-aafc-a1895de5ac4d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-16T00:38:15.252032Z","iopub.execute_input":"2023-04-16T00:38:15.255452Z","iopub.status.idle":"2023-04-16T00:38:25.822901Z","shell.execute_reply.started":"2023-04-16T00:38:15.255405Z","shell.execute_reply":"2023-04-16T00:38:25.821595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nimport torch.nn.functional as F\n\ndef normalize(images):\n    return ((images / 255 - NORM_MEAN) / NORM_STD)\n    \ndef TrainEpoch(model, loader, epoch):\n    print(\"Starting Training (Epoch %d):\" % epoch)\n    \n    model.train()\n    \n    loss_sum = 0\n    loss_count = 0\n    \n    for i, data in enumerate(loader, 1):\n        inputs, labels = data\n        \n        inputs = inputs.to(DEVICE, non_blocking = True)\n        inputs = normalize(inputs)\n        \n#         inputs = transform(inputs)\n        \n        labels = labels.to(DEVICE, non_blocking = True)   \n        \n        optimizer.zero_grad(set_to_none = True)\n        \n        with torch.autocast(device_type = DEVICE, dtype = torch.float16):\n            outputs = model(inputs)\n            \n            loss = criterion(outputs, labels)\n        \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        loss_sum += loss.item() * inputs.shape[0]\n        loss_count += inputs.shape[0]\n        \n#         lr_scheduler.step(epoch + i / len(loader))\n        \n    print(\"Training Loss Average (Epoch %d): %.6f\" % (epoch, loss_sum / loss_count))\n    \n    run.log({\"Training Loss\" : loss_sum / loss_count})\n    \ndef ValidEpoch(model, loader, epoch):\n    print(\"Starting Validation (Epoch %d):\" % epoch)\n    \n    model.eval()\n    \n    loss_sum = 0\n    loss_count = 0\n    \n    metrics.reset()\n    \n    with torch.no_grad():\n        for i, data in enumerate(loader, 1):\n            inputs, labels = data\n\n            inputs = inputs.to(DEVICE, non_blocking = True)\n            inputs = normalize(inputs)\n            \n#             inputs = transform(inputs)\n            \n            labels = labels.to(DEVICE, non_blocking = True)   \n            \n            with torch.autocast(device_type = DEVICE, dtype = torch.float16):\n                outputs = model(inputs)\n\n                loss = criterion(outputs, labels)\n                \n            loss_sum += loss.item() * inputs.shape[0]\n            loss_count += inputs.shape[0]\n            \n            metrics(outputs.argmax(1), labels)\n            \n    all_metrics = metrics.compute()\n            \n    print(\"Validation (Epoch %d):\" % (epoch))\n    print(\"Loss: %.6f, Precision: %.6f, Recall: %.6f, F1 Score: %.6f\" % (loss_sum / loss_count, all_metrics['MulticlassPrecision'], all_metrics['MulticlassRecall'], all_metrics['MulticlassF1Score']))\n    \n    print(all_metrics['MulticlassConfusionMatrix'])\n    \n    run.log({\"Validation Loss\" : loss_sum / loss_count})\n    run.log({\"Precision\" : all_metrics['MulticlassPrecision']})\n    run.log({\"Recall\" : all_metrics['MulticlassRecall']})\n    run.log({\"F1 Score\" : all_metrics['MulticlassF1Score']})\n    \n    return (loss_sum / loss_count, all_metrics['MulticlassF1Score'])","metadata":{"_uuid":"83c3f7f9-4b3d-480c-b6e0-2c62ac3a18e0","_cell_guid":"9822ff78-6f62-489e-976e-857939fbf5b4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-16T20:55:56.495554Z","iopub.execute_input":"2023-04-16T20:55:56.496031Z","iopub.status.idle":"2023-04-16T20:55:56.515702Z","shell.execute_reply.started":"2023-04-16T20:55:56.495987Z","shell.execute_reply":"2023-04-16T20:55:56.514395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy\nimport math\nimport warnings\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union\n\nimport torch\nfrom torch import nn, Tensor\nfrom torchvision.ops import StochasticDepth\n\nfrom torchvision.ops.misc import Conv2dNormActivation, SqueezeExcitation\nfrom torchvision.transforms._presets import ImageClassification, InterpolationMode\nfrom torchvision.utils import _log_api_usage_once\nfrom torchvision.models._api import Weights, WeightsEnum\nfrom torchvision.models._meta import _IMAGENET_CATEGORIES\nfrom torchvision.models._utils import _make_divisible, _ovewrite_named_param, handle_legacy_interface\n\n\n__all__ = [\n    \"EfficientNet\",\n    \"EfficientNet_B0_Weights\",\n    \"EfficientNet_B1_Weights\",\n    \"EfficientNet_B2_Weights\",\n    \"EfficientNet_B3_Weights\",\n    \"EfficientNet_B4_Weights\",\n    \"EfficientNet_B5_Weights\",\n    \"EfficientNet_B6_Weights\",\n    \"EfficientNet_B7_Weights\",\n    \"EfficientNet_V2_S_Weights\",\n    \"EfficientNet_V2_M_Weights\",\n    \"EfficientNet_V2_L_Weights\",\n    \"efficientnet_b0\",\n    \"efficientnet_b1\",\n    \"efficientnet_b2\",\n    \"efficientnet_b3\",\n    \"efficientnet_b4\",\n    \"efficientnet_b5\",\n    \"efficientnet_b6\",\n    \"efficientnet_b7\",\n    \"efficientnet_v2_s\",\n    \"efficientnet_v2_m\",\n    \"efficientnet_v2_l\",\n]\n\n\n@dataclass\nclass _MBConvConfig:\n    expand_ratio: float\n    kernel: int\n    stride: int\n    input_channels: int\n    out_channels: int\n    num_layers: int\n    block: Callable[..., nn.Module]\n\n    @staticmethod\n    def adjust_channels(channels: int, width_mult: float, min_value: Optional[int] = None) -> int:\n        return _make_divisible(channels * width_mult, 8, min_value)\n\n\nclass MBConvConfig(_MBConvConfig):\n    # Stores information listed at Table 1 of the EfficientNet paper & Table 4 of the EfficientNetV2 paper\n    def __init__(\n        self,\n        expand_ratio: float,\n        kernel: int,\n        stride: int,\n        input_channels: int,\n        out_channels: int,\n        num_layers: int,\n        width_mult: float = 1.0,\n        depth_mult: float = 1.0,\n        block: Optional[Callable[..., nn.Module]] = None,\n    ) -> None:\n        input_channels = self.adjust_channels(input_channels, width_mult)\n        out_channels = self.adjust_channels(out_channels, width_mult)\n        num_layers = self.adjust_depth(num_layers, depth_mult)\n        if block is None:\n            block = MBConv\n        super().__init__(expand_ratio, kernel, stride, input_channels, out_channels, num_layers, block)\n\n    @staticmethod\n    def adjust_depth(num_layers: int, depth_mult: float):\n        return int(math.ceil(num_layers * depth_mult))\n\n\nclass FusedMBConvConfig(_MBConvConfig):\n    # Stores information listed at Table 4 of the EfficientNetV2 paper\n    def __init__(\n        self,\n        expand_ratio: float,\n        kernel: int,\n        stride: int,\n        input_channels: int,\n        out_channels: int,\n        num_layers: int,\n        block: Optional[Callable[..., nn.Module]] = None,\n    ) -> None:\n        if block is None:\n            block = FusedMBConv\n        super().__init__(expand_ratio, kernel, stride, input_channels, out_channels, num_layers, block)\n\n\nclass MBConv(nn.Module):\n    def __init__(\n        self,\n        cnf: MBConvConfig,\n        stochastic_depth_prob: float,\n        norm_layer: Callable[..., nn.Module],\n        se_layer: Callable[..., nn.Module] = SqueezeExcitation,\n    ) -> None:\n        super().__init__()\n\n        if not (1 <= cnf.stride <= 2):\n            raise ValueError(\"illegal stride value\")\n\n        self.use_res_connect = cnf.stride == 1 and cnf.input_channels == cnf.out_channels\n\n        layers: List[nn.Module] = []\n        activation_layer = nn.SiLU\n\n        # expand\n        expanded_channels = cnf.adjust_channels(cnf.input_channels, cnf.expand_ratio)\n        if expanded_channels != cnf.input_channels:\n            layers.append(\n                Conv2dNormActivation(\n                    cnf.input_channels,\n                    expanded_channels,\n                    kernel_size=1,\n                    norm_layer=norm_layer,\n                    activation_layer=activation_layer,\n                )\n            )\n\n        # depthwise\n        layers.append(\n            Conv2dNormActivation(\n                expanded_channels,\n                expanded_channels,\n                kernel_size=cnf.kernel,\n                stride=cnf.stride,\n                groups=expanded_channels,\n                norm_layer=norm_layer,\n                activation_layer=activation_layer,\n            )\n        )\n\n        # squeeze and excitation\n        squeeze_channels = max(1, cnf.input_channels // 4)\n        layers.append(se_layer(expanded_channels, squeeze_channels, activation=partial(nn.SiLU, inplace=True)))\n\n        # project\n        layers.append(\n            Conv2dNormActivation(\n                expanded_channels, cnf.out_channels, kernel_size=1, norm_layer=norm_layer, activation_layer=None\n            )\n        )\n\n        self.block = nn.Sequential(*layers)\n        self.stochastic_depth = StochasticDepth(stochastic_depth_prob, \"row\")\n        self.out_channels = cnf.out_channels\n\n    def forward(self, input: Tensor) -> Tensor:\n        result = self.block(input)\n        if self.use_res_connect:\n            result = self.stochastic_depth(result)\n            result += input\n        return result\n\n\nclass FusedMBConv(nn.Module):\n    def __init__(\n        self,\n        cnf: FusedMBConvConfig,\n        stochastic_depth_prob: float,\n        norm_layer: Callable[..., nn.Module],\n    ) -> None:\n        super().__init__()\n\n        if not (1 <= cnf.stride <= 2):\n            raise ValueError(\"illegal stride value\")\n\n        self.use_res_connect = cnf.stride == 1 and cnf.input_channels == cnf.out_channels\n\n        layers: List[nn.Module] = []\n        activation_layer = nn.SiLU\n\n        expanded_channels = cnf.adjust_channels(cnf.input_channels, cnf.expand_ratio)\n        if expanded_channels != cnf.input_channels:\n            # fused expand\n            layers.append(\n                Conv2dNormActivation(\n                    cnf.input_channels,\n                    expanded_channels,\n                    kernel_size=cnf.kernel,\n                    stride=cnf.stride,\n                    norm_layer=norm_layer,\n                    activation_layer=activation_layer,\n                )\n            )\n\n            # project\n            layers.append(\n                Conv2dNormActivation(\n                    expanded_channels, cnf.out_channels, kernel_size=1, norm_layer=norm_layer, activation_layer=None\n                )\n            )\n        else:\n            layers.append(\n                Conv2dNormActivation(\n                    cnf.input_channels,\n                    cnf.out_channels,\n                    kernel_size=cnf.kernel,\n                    stride=cnf.stride,\n                    norm_layer=norm_layer,\n                    activation_layer=activation_layer,\n                )\n            )\n\n        self.block = nn.Sequential(*layers)\n        self.stochastic_depth = StochasticDepth(stochastic_depth_prob, \"row\")\n        self.out_channels = cnf.out_channels\n\n    def forward(self, input: Tensor) -> Tensor:\n        result = self.block(input)\n        if self.use_res_connect:\n            result = self.stochastic_depth(result)\n            result += input\n        return result\n\n\nclass EfficientNet(nn.Module):\n    def __init__(\n        self,\n        inverted_residual_setting: Sequence[Union[MBConvConfig, FusedMBConvConfig]],\n        dropout: float,\n        stochastic_depth_prob: float = 0.2,\n        num_classes: int = 1000,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n        last_channel: Optional[int] = None,\n    ) -> None:\n        \"\"\"\n        EfficientNet V1 and V2 main class\n\n        Args:\n            inverted_residual_setting (Sequence[Union[MBConvConfig, FusedMBConvConfig]]): Network structure\n            dropout (float): The droupout probability\n            stochastic_depth_prob (float): The stochastic depth probability\n            num_classes (int): Number of classes\n            norm_layer (Optional[Callable[..., nn.Module]]): Module specifying the normalization layer to use\n            last_channel (int): The number of channels on the penultimate layer\n        \"\"\"\n        super().__init__()\n        _log_api_usage_once(self)\n\n        if not inverted_residual_setting:\n            raise ValueError(\"The inverted_residual_setting should not be empty\")\n        elif not (\n            isinstance(inverted_residual_setting, Sequence)\n            and all([isinstance(s, _MBConvConfig) for s in inverted_residual_setting])\n        ):\n            raise TypeError(\"The inverted_residual_setting should be List[MBConvConfig]\")\n\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n\n        layers: List[nn.Module] = []\n\n        # building first layer\n        firstconv_output_channels = inverted_residual_setting[0].input_channels\n        layers.append(\n            Conv2dNormActivation(\n                4, firstconv_output_channels, kernel_size=3, stride=2, norm_layer=norm_layer, activation_layer=nn.SiLU\n            )\n        )\n\n        # building inverted residual blocks\n        total_stage_blocks = sum(cnf.num_layers for cnf in inverted_residual_setting)\n        stage_block_id = 0\n        for cnf in inverted_residual_setting:\n            stage: List[nn.Module] = []\n            for _ in range(cnf.num_layers):\n                # copy to avoid modifications. shallow copy is enough\n                block_cnf = copy.copy(cnf)\n\n                # overwrite info if not the first conv in the stage\n                if stage:\n                    block_cnf.input_channels = block_cnf.out_channels\n                    block_cnf.stride = 1\n\n                # adjust stochastic depth probability based on the depth of the stage block\n                sd_prob = stochastic_depth_prob * float(stage_block_id) / total_stage_blocks\n\n                stage.append(block_cnf.block(block_cnf, sd_prob, norm_layer))\n                stage_block_id += 1\n\n            layers.append(nn.Sequential(*stage))\n\n        # building last several layers\n        lastconv_input_channels = inverted_residual_setting[-1].out_channels\n        lastconv_output_channels = last_channel if last_channel is not None else 4 * lastconv_input_channels\n        layers.append(\n            Conv2dNormActivation(\n                lastconv_input_channels,\n                lastconv_output_channels,\n                kernel_size=1,\n                norm_layer=norm_layer,\n                activation_layer=nn.SiLU,\n            )\n        )\n\n        self.features = nn.Sequential(*layers)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=dropout, inplace=True),\n            nn.Linear(lastconv_output_channels, num_classes),\n        )\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                init_range = 1.0 / math.sqrt(m.out_features)\n                nn.init.uniform_(m.weight, -init_range, init_range)\n                nn.init.zeros_(m.bias)\n\n    def _forward_impl(self, x: Tensor) -> Tensor:\n        x = self.features(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n\n        x = self.classifier(x)\n\n        return x\n\n    def forward(self, x: Tensor) -> Tensor:\n        return self._forward_impl(x)\n\n\ndef _efficientnet(\n    inverted_residual_setting: Sequence[Union[MBConvConfig, FusedMBConvConfig]],\n    dropout: float,\n    last_channel: Optional[int],\n    weights: Optional[WeightsEnum],\n    progress: bool,\n    **kwargs: Any,\n) -> EfficientNet:\n#     if weights is not None:\n#         _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n\n    model = EfficientNet(inverted_residual_setting, dropout, last_channel=last_channel, **kwargs)\n\n    if weights is not None:\n        state_dict = weights.get_state_dict(progress=progress)\n        \n        state_dict['features.0.0.weight'] = state_dict['features.0.0.weight'][:, :1].expand(-1, 4, -1, -1)\n        state_dict['classifier.1.weight'] = state_dict['classifier.1.weight'][:CLASS_COUNT, :]\n        state_dict['classifier.1.bias'] = state_dict['classifier.1.bias'][:CLASS_COUNT]\n        \n        model.load_state_dict(state_dict)\n\n    return model\n\n\ndef _efficientnet_conf(\n    arch: str,\n    **kwargs: Any,\n) -> Tuple[Sequence[Union[MBConvConfig, FusedMBConvConfig]], Optional[int]]:\n    inverted_residual_setting: Sequence[Union[MBConvConfig, FusedMBConvConfig]]\n    if arch.startswith(\"efficientnet_b\"):\n        bneck_conf = partial(MBConvConfig, width_mult=kwargs.pop(\"width_mult\"), depth_mult=kwargs.pop(\"depth_mult\"))\n        inverted_residual_setting = [\n            bneck_conf(1, 3, 1, 32, 16, 1),\n            bneck_conf(6, 3, 2, 16, 24, 2),\n            bneck_conf(6, 5, 2, 24, 40, 2),\n            bneck_conf(6, 3, 2, 40, 80, 3),\n            bneck_conf(6, 5, 1, 80, 112, 3),\n            bneck_conf(6, 5, 2, 112, 192, 4),\n            bneck_conf(6, 3, 1, 192, 320, 1),\n        ]\n        last_channel = None\n    elif arch.startswith(\"efficientnet_v2_s\"):\n        inverted_residual_setting = [\n            FusedMBConvConfig(1, 3, 1, 24, 24, 2),\n            FusedMBConvConfig(4, 3, 2, 24, 48, 4),\n            FusedMBConvConfig(4, 3, 2, 48, 64, 4),\n            MBConvConfig(4, 3, 2, 64, 128, 6),\n            MBConvConfig(6, 3, 1, 128, 160, 9),\n            MBConvConfig(6, 3, 2, 160, 256, 15),\n        ]\n        last_channel = 1280\n    elif arch.startswith(\"efficientnet_v2_m\"):\n        inverted_residual_setting = [\n            FusedMBConvConfig(1, 3, 1, 24, 24, 3),\n            FusedMBConvConfig(4, 3, 2, 24, 48, 5),\n            FusedMBConvConfig(4, 3, 2, 48, 80, 5),\n            MBConvConfig(4, 3, 2, 80, 160, 7),\n            MBConvConfig(6, 3, 1, 160, 176, 14),\n            MBConvConfig(6, 3, 2, 176, 304, 18),\n            MBConvConfig(6, 3, 1, 304, 512, 5),\n        ]\n        last_channel = 1280\n    elif arch.startswith(\"efficientnet_v2_l\"):\n        inverted_residual_setting = [\n            FusedMBConvConfig(1, 3, 1, 32, 32, 4),\n            FusedMBConvConfig(4, 3, 2, 32, 64, 7),\n            FusedMBConvConfig(4, 3, 2, 64, 96, 7),\n            MBConvConfig(4, 3, 2, 96, 192, 10),\n            MBConvConfig(6, 3, 1, 192, 224, 19),\n            MBConvConfig(6, 3, 2, 224, 384, 25),\n            MBConvConfig(6, 3, 1, 384, 640, 7),\n        ]\n        last_channel = 1280\n    else:\n        raise ValueError(f\"Unsupported model type {arch}\")\n\n    return inverted_residual_setting, last_channel\n\n\n_COMMON_META: Dict[str, Any] = {\n    \"categories\": _IMAGENET_CATEGORIES,\n}\n\n\n_COMMON_META_V1 = {\n    **_COMMON_META,\n    \"min_size\": (1, 1),\n    \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1\",\n}\n\n\n_COMMON_META_V2 = {\n    **_COMMON_META,\n    \"min_size\": (33, 33),\n    \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v2\",\n}\n\n\nclass EfficientNet_B0_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        # Weights ported from https://github.com/rwightman/pytorch-image-models/\n        url=\"https://download.pytorch.org/models/efficientnet_b0_rwightman-3dd342df.pth\",\n        transforms=partial(\n            ImageClassification, crop_size=224, resize_size=256, interpolation=InterpolationMode.BICUBIC\n        ),\n        meta={\n            **_COMMON_META_V1,\n            \"num_params\": 5288548,\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 77.692,\n                    \"acc@5\": 93.532,\n                }\n            },\n            \"_ops\": 0.386,\n            \"_file_size\": 20.451,\n            \"_docs\": \"\"\"These weights are ported from the original paper.\"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\nclass EfficientNet_B1_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        # Weights ported from https://github.com/rwightman/pytorch-image-models/\n        url=\"https://download.pytorch.org/models/efficientnet_b1_rwightman-533bc792.pth\",\n        transforms=partial(\n            ImageClassification, crop_size=240, resize_size=256, interpolation=InterpolationMode.BICUBIC\n        ),\n        meta={\n            **_COMMON_META_V1,\n            \"num_params\": 7794184,\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 78.642,\n                    \"acc@5\": 94.186,\n                }\n            },\n            \"_ops\": 0.687,\n            \"_file_size\": 30.134,\n            \"_docs\": \"\"\"These weights are ported from the original paper.\"\"\",\n        },\n    )\n    IMAGENET1K_V2 = Weights(\n        url=\"https://download.pytorch.org/models/efficientnet_b1-c27df63c.pth\",\n        transforms=partial(\n            ImageClassification, crop_size=240, resize_size=255, interpolation=InterpolationMode.BILINEAR\n        ),\n        meta={\n            **_COMMON_META_V1,\n            \"num_params\": 7794184,\n            \"recipe\": \"https://github.com/pytorch/vision/issues/3995#new-recipe-with-lr-wd-crop-tuning\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 79.838,\n                    \"acc@5\": 94.934,\n                }\n            },\n            \"_ops\": 0.687,\n            \"_file_size\": 30.136,\n            \"_docs\": \"\"\"\n                These weights improve upon the results of the original paper by using a modified version of TorchVision's\n                `new training recipe\n                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n            \"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V2\n\n\nclass EfficientNet_B2_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        # Weights ported from https://github.com/rwightman/pytorch-image-models/\n        url=\"https://download.pytorch.org/models/efficientnet_b2_rwightman-bcdf34b7.pth\",\n        transforms=partial(\n            ImageClassification, crop_size=288, resize_size=288, interpolation=InterpolationMode.BICUBIC\n        ),\n        meta={\n            **_COMMON_META_V1,\n            \"num_params\": 9109994,\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 80.608,\n                    \"acc@5\": 95.310,\n                }\n            },\n            \"_ops\": 1.088,\n            \"_file_size\": 35.174,\n            \"_docs\": \"\"\"These weights are ported from the original paper.\"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\nclass EfficientNet_B3_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        # Weights ported from https://github.com/rwightman/pytorch-image-models/\n        url=\"https://download.pytorch.org/models/efficientnet_b3_rwightman-cf984f9c.pth\",\n        transforms=partial(\n            ImageClassification, crop_size=300, resize_size=320, interpolation=InterpolationMode.BICUBIC\n        ),\n        meta={\n            **_COMMON_META_V1,\n            \"num_params\": 12233232,\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 82.008,\n                    \"acc@5\": 96.054,\n                }\n            },\n            \"_ops\": 1.827,\n            \"_file_size\": 47.184,\n            \"_docs\": \"\"\"These weights are ported from the original paper.\"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\nclass EfficientNet_B4_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        # Weights ported from https://github.com/rwightman/pytorch-image-models/\n        url=\"https://download.pytorch.org/models/efficientnet_b4_rwightman-7eb33cd5.pth\",\n        transforms=partial(\n            ImageClassification, crop_size=380, resize_size=384, interpolation=InterpolationMode.BICUBIC\n        ),\n        meta={\n            **_COMMON_META_V1,\n            \"num_params\": 19341616,\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 83.384,\n                    \"acc@5\": 96.594,\n                }\n            },\n            \"_ops\": 4.394,\n            \"_file_size\": 74.489,\n            \"_docs\": \"\"\"These weights are ported from the original paper.\"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\nclass EfficientNet_B5_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        # Weights ported from https://github.com/lukemelas/EfficientNet-PyTorch/\n        url=\"https://download.pytorch.org/models/efficientnet_b5_lukemelas-b6417697.pth\",\n        transforms=partial(\n            ImageClassification, crop_size=456, resize_size=456, interpolation=InterpolationMode.BICUBIC\n        ),\n        meta={\n            **_COMMON_META_V1,\n            \"num_params\": 30389784,\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 83.444,\n                    \"acc@5\": 96.628,\n                }\n            },\n            \"_ops\": 10.266,\n            \"_file_size\": 116.864,\n            \"_docs\": \"\"\"These weights are ported from the original paper.\"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\nclass EfficientNet_B6_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        # Weights ported from https://github.com/lukemelas/EfficientNet-PyTorch/\n        url=\"https://download.pytorch.org/models/efficientnet_b6_lukemelas-c76e70fd.pth\",\n        transforms=partial(\n            ImageClassification, crop_size=528, resize_size=528, interpolation=InterpolationMode.BICUBIC\n        ),\n        meta={\n            **_COMMON_META_V1,\n            \"num_params\": 43040704,\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 84.008,\n                    \"acc@5\": 96.916,\n                }\n            },\n            \"_ops\": 19.068,\n            \"_file_size\": 165.362,\n            \"_docs\": \"\"\"These weights are ported from the original paper.\"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\nclass EfficientNet_B7_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        # Weights ported from https://github.com/lukemelas/EfficientNet-PyTorch/\n        url=\"https://download.pytorch.org/models/efficientnet_b7_lukemelas-dcc49843.pth\",\n        transforms=partial(\n            ImageClassification, crop_size=600, resize_size=600, interpolation=InterpolationMode.BICUBIC\n        ),\n        meta={\n            **_COMMON_META_V1,\n            \"num_params\": 66347960,\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 84.122,\n                    \"acc@5\": 96.908,\n                }\n            },\n            \"_ops\": 37.746,\n            \"_file_size\": 254.675,\n            \"_docs\": \"\"\"These weights are ported from the original paper.\"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\nclass EfficientNet_V2_S_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        url=\"https://download.pytorch.org/models/efficientnet_v2_s-dd5fe13b.pth\",\n        transforms=partial(\n            ImageClassification,\n            crop_size=384,\n            resize_size=384,\n            interpolation=InterpolationMode.BILINEAR,\n        ),\n        meta={\n            **_COMMON_META_V2,\n            \"num_params\": 21458488,\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 84.228,\n                    \"acc@5\": 96.878,\n                }\n            },\n            \"_ops\": 8.366,\n            \"_file_size\": 82.704,\n            \"_docs\": \"\"\"\n                These weights improve upon the results of the original paper by using a modified version of TorchVision's\n                `new training recipe\n                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n            \"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\nclass EfficientNet_V2_M_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        url=\"https://download.pytorch.org/models/efficientnet_v2_m-dc08266a.pth\",\n        transforms=partial(\n            ImageClassification,\n            crop_size=480,\n            resize_size=480,\n            interpolation=InterpolationMode.BILINEAR,\n        ),\n        meta={\n            **_COMMON_META_V2,\n            \"num_params\": 54139356,\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 85.112,\n                    \"acc@5\": 97.156,\n                }\n            },\n            \"_ops\": 24.582,\n            \"_file_size\": 208.01,\n            \"_docs\": \"\"\"\n                These weights improve upon the results of the original paper by using a modified version of TorchVision's\n                `new training recipe\n                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n            \"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\nclass EfficientNet_V2_L_Weights(WeightsEnum):\n    # Weights ported from https://github.com/google/automl/tree/master/efficientnetv2\n    IMAGENET1K_V1 = Weights(\n        url=\"https://download.pytorch.org/models/efficientnet_v2_l-59c71312.pth\",\n        transforms=partial(\n            ImageClassification,\n            crop_size=480,\n            resize_size=480,\n            interpolation=InterpolationMode.BICUBIC,\n            mean=(0.5, 0.5, 0.5),\n            std=(0.5, 0.5, 0.5),\n        ),\n        meta={\n            **_COMMON_META_V2,\n            \"num_params\": 118515272,\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 85.808,\n                    \"acc@5\": 97.788,\n                }\n            },\n            \"_ops\": 56.08,\n            \"_file_size\": 454.573,\n            \"_docs\": \"\"\"These weights are ported from the original paper.\"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\n@handle_legacy_interface(weights=(\"pretrained\", EfficientNet_B0_Weights.IMAGENET1K_V1))\ndef efficientnet_b0(\n    *, weights: Optional[EfficientNet_B0_Weights] = None, progress: bool = True, **kwargs: Any\n) -> EfficientNet:\n    \"\"\"EfficientNet B0 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional\n    Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper.\n\n    Args:\n        weights (:class:`~torchvision.models.EfficientNet_B0_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.EfficientNet_B0_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.EfficientNet_B0_Weights\n        :members:\n    \"\"\"\n    weights = EfficientNet_B0_Weights.verify(weights)\n\n    inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_b0\", width_mult=1.0, depth_mult=1.0)\n    return _efficientnet(\n        inverted_residual_setting, kwargs.pop(\"dropout\", 0.2), last_channel, weights, progress, **kwargs\n    )\n\n\n@handle_legacy_interface(weights=(\"pretrained\", EfficientNet_B1_Weights.IMAGENET1K_V1))\ndef efficientnet_b1(\n    *, weights: Optional[EfficientNet_B1_Weights] = None, progress: bool = True, **kwargs: Any\n) -> EfficientNet:\n    \"\"\"EfficientNet B1 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional\n    Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper.\n\n    Args:\n        weights (:class:`~torchvision.models.EfficientNet_B1_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.EfficientNet_B1_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.EfficientNet_B1_Weights\n        :members:\n    \"\"\"\n    weights = EfficientNet_B1_Weights.verify(weights)\n\n    inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_b1\", width_mult=1.0, depth_mult=1.1)\n    return _efficientnet(\n        inverted_residual_setting, kwargs.pop(\"dropout\", 0.2), last_channel, weights, progress, **kwargs\n    )\n\n\n@handle_legacy_interface(weights=(\"pretrained\", EfficientNet_B2_Weights.IMAGENET1K_V1))\ndef efficientnet_b2(\n    *, weights: Optional[EfficientNet_B2_Weights] = None, progress: bool = True, **kwargs: Any\n) -> EfficientNet:\n    \"\"\"EfficientNet B2 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional\n    Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper.\n\n    Args:\n        weights (:class:`~torchvision.models.EfficientNet_B2_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.EfficientNet_B2_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.EfficientNet_B2_Weights\n        :members:\n    \"\"\"\n    weights = EfficientNet_B2_Weights.verify(weights)\n\n    inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_b2\", width_mult=1.1, depth_mult=1.2)\n    return _efficientnet(\n        inverted_residual_setting, kwargs.pop(\"dropout\", 0.3), last_channel, weights, progress, **kwargs\n    )\n\n\n@handle_legacy_interface(weights=(\"pretrained\", EfficientNet_B3_Weights.IMAGENET1K_V1))\ndef efficientnet_b3(\n    *, weights: Optional[EfficientNet_B3_Weights] = None, progress: bool = True, **kwargs: Any\n) -> EfficientNet:\n    \"\"\"EfficientNet B3 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional\n    Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper.\n\n    Args:\n        weights (:class:`~torchvision.models.EfficientNet_B3_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.EfficientNet_B3_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.EfficientNet_B3_Weights\n        :members:\n    \"\"\"\n    weights = EfficientNet_B3_Weights.verify(weights)\n\n    inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_b3\", width_mult=1.2, depth_mult=1.4)\n    return _efficientnet(\n        inverted_residual_setting,\n        kwargs.pop(\"dropout\", 0.3),\n        last_channel,\n        weights,\n        progress,\n        **kwargs,\n    )\n\n\n@handle_legacy_interface(weights=(\"pretrained\", EfficientNet_B4_Weights.IMAGENET1K_V1))\ndef efficientnet_b4(\n    *, weights: Optional[EfficientNet_B4_Weights] = None, progress: bool = True, **kwargs: Any\n) -> EfficientNet:\n    \"\"\"EfficientNet B4 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional\n    Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper.\n\n    Args:\n        weights (:class:`~torchvision.models.EfficientNet_B4_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.EfficientNet_B4_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.EfficientNet_B4_Weights\n        :members:\n    \"\"\"\n    weights = EfficientNet_B4_Weights.verify(weights)\n\n    inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_b4\", width_mult=1.4, depth_mult=1.8)\n    return _efficientnet(\n        inverted_residual_setting,\n        kwargs.pop(\"dropout\", 0.4),\n        last_channel,\n        weights,\n        progress,\n        **kwargs,\n    )\n\n\n\n@handle_legacy_interface(weights=(\"pretrained\", EfficientNet_B5_Weights.IMAGENET1K_V1))\ndef efficientnet_b5(\n    *, weights: Optional[EfficientNet_B5_Weights] = None, progress: bool = True, **kwargs: Any\n) -> EfficientNet:\n    \"\"\"EfficientNet B5 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional\n    Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper.\n\n    Args:\n        weights (:class:`~torchvision.models.EfficientNet_B5_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.EfficientNet_B5_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.EfficientNet_B5_Weights\n        :members:\n    \"\"\"\n    weights = EfficientNet_B5_Weights.verify(weights)\n\n    inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_b5\", width_mult=1.6, depth_mult=2.2)\n    return _efficientnet(\n        inverted_residual_setting,\n        kwargs.pop(\"dropout\", 0.4),\n        last_channel,\n        weights,\n        progress,\n        norm_layer=partial(nn.BatchNorm2d, eps=0.001, momentum=0.01),\n        **kwargs,\n    )\n\n\n\n@handle_legacy_interface(weights=(\"pretrained\", EfficientNet_B6_Weights.IMAGENET1K_V1))\ndef efficientnet_b6(\n    *, weights: Optional[EfficientNet_B6_Weights] = None, progress: bool = True, **kwargs: Any\n) -> EfficientNet:\n    \"\"\"EfficientNet B6 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional\n    Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper.\n\n    Args:\n        weights (:class:`~torchvision.models.EfficientNet_B6_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.EfficientNet_B6_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.EfficientNet_B6_Weights\n        :members:\n    \"\"\"\n    weights = EfficientNet_B6_Weights.verify(weights)\n\n    inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_b6\", width_mult=1.8, depth_mult=2.6)\n    return _efficientnet(\n        inverted_residual_setting,\n        kwargs.pop(\"dropout\", 0.5),\n        last_channel,\n        weights,\n        progress,\n        norm_layer=partial(nn.BatchNorm2d, eps=0.001, momentum=0.01),\n        **kwargs,\n    )\n\n\n\n@handle_legacy_interface(weights=(\"pretrained\", EfficientNet_B7_Weights.IMAGENET1K_V1))\ndef efficientnet_b7(\n    *, weights: Optional[EfficientNet_B7_Weights] = None, progress: bool = True, **kwargs: Any\n) -> EfficientNet:\n    \"\"\"EfficientNet B7 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional\n    Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper.\n\n    Args:\n        weights (:class:`~torchvision.models.EfficientNet_B7_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.EfficientNet_B7_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.EfficientNet_B7_Weights\n        :members:\n    \"\"\"\n    weights = EfficientNet_B7_Weights.verify(weights)\n\n    inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_b7\", width_mult=2.0, depth_mult=3.1)\n    return _efficientnet(\n        inverted_residual_setting,\n        kwargs.pop(\"dropout\", 0.5),\n        last_channel,\n        weights,\n        progress,\n        norm_layer=partial(nn.BatchNorm2d, eps=0.001, momentum=0.01),\n        **kwargs,\n    )\n\n\n\n@handle_legacy_interface(weights=(\"pretrained\", EfficientNet_V2_S_Weights.IMAGENET1K_V1))\ndef efficientnet_v2_s(\n    *, weights: Optional[EfficientNet_V2_S_Weights] = None, progress: bool = True, **kwargs: Any\n) -> EfficientNet:\n    \"\"\"\n    Constructs an EfficientNetV2-S architecture from\n    `EfficientNetV2: Smaller Models and Faster Training <https://arxiv.org/abs/2104.00298>`_.\n\n    Args:\n        weights (:class:`~torchvision.models.EfficientNet_V2_S_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.EfficientNet_V2_S_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.EfficientNet_V2_S_Weights\n        :members:\n    \"\"\"\n    weights = EfficientNet_V2_S_Weights.verify(weights)\n\n    inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_v2_s\")\n    return _efficientnet(\n        inverted_residual_setting,\n        kwargs.pop(\"dropout\", 0.2),\n        last_channel,\n        weights,\n        progress,\n        norm_layer=partial(nn.BatchNorm2d, eps=1e-03),\n        **kwargs,\n    )\n\n\n\n@handle_legacy_interface(weights=(\"pretrained\", EfficientNet_V2_M_Weights.IMAGENET1K_V1))\ndef efficientnet_v2_m(\n    *, weights: Optional[EfficientNet_V2_M_Weights] = None, progress: bool = True, **kwargs: Any\n) -> EfficientNet:\n    \"\"\"\n    Constructs an EfficientNetV2-M architecture from\n    `EfficientNetV2: Smaller Models and Faster Training <https://arxiv.org/abs/2104.00298>`_.\n\n    Args:\n        weights (:class:`~torchvision.models.EfficientNet_V2_M_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.EfficientNet_V2_M_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.EfficientNet_V2_M_Weights\n        :members:\n    \"\"\"\n    weights = EfficientNet_V2_M_Weights.verify(weights)\n\n    inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_v2_m\")\n    return _efficientnet(\n        inverted_residual_setting,\n        kwargs.pop(\"dropout\", 0.3),\n        last_channel,\n        weights,\n        progress,\n        norm_layer=partial(nn.BatchNorm2d, eps=1e-03),\n        **kwargs,\n    )\n\n\n\n@handle_legacy_interface(weights=(\"pretrained\", EfficientNet_V2_L_Weights.IMAGENET1K_V1))\ndef efficientnet_v2_l(\n    *, weights: Optional[EfficientNet_V2_L_Weights] = None, progress: bool = True, **kwargs: Any\n) -> EfficientNet:\n    \"\"\"\n    Constructs an EfficientNetV2-L architecture from\n    `EfficientNetV2: Smaller Models and Faster Training <https://arxiv.org/abs/2104.00298>`_.\n\n    Args:\n        weights (:class:`~torchvision.models.EfficientNet_V2_L_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.EfficientNet_V2_L_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.EfficientNet_V2_L_Weights\n        :members:\n    \"\"\"\n    weights = EfficientNet_V2_L_Weights.verify(weights)\n\n    inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_v2_l\")\n    return _efficientnet(\n        inverted_residual_setting,\n        kwargs.pop(\"dropout\", 0.4),\n        last_channel,\n        weights,\n        progress,\n        norm_layer=partial(nn.BatchNorm2d, eps=1e-03),\n        **kwargs,\n    )","metadata":{"_uuid":"9c0e947c-60c5-4001-9b3a-b59a639b8f10","_cell_guid":"2386fb37-6589-4150-bf1b-bc7daeb1667d","execution":{"iopub.status.busy":"2023-04-22T21:57:38.382147Z","iopub.execute_input":"2023-04-22T21:57:38.382567Z","iopub.status.idle":"2023-04-22T21:57:41.819813Z","shell.execute_reply.started":"2023-04-22T21:57:38.382531Z","shell.execute_reply":"2023-04-22T21:57:41.818797Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"wandb_alperent\")\n\n!wandb login $api_key","metadata":{"_uuid":"02806b50-ec44-4195-b4eb-98811ee0734b","_cell_guid":"37ad5ded-f597-46ab-a832-9401d2e83b88","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-16T00:38:25.963528Z","iopub.execute_input":"2023-04-16T00:38:25.963900Z","iopub.status.idle":"2023-04-16T00:38:30.933835Z","shell.execute_reply.started":"2023-04-16T00:38:25.963863Z","shell.execute_reply":"2023-04-16T00:38:30.932123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torchvision\n\ntorch.cuda.empty_cache()\n\n# model = efficientnet_v2_s(weights = torchvision.models.EfficientNet_V2_S_Weights, num_classes = CLASS_COUNT)\n\n# model = efficientnet_v2_s(weights = torchvision.models.EfficientNet_V2_S_Weights, num_classes = CLASS_COUNT, dropout = 0.5)\n\n# transform = torchvision.transforms.Resize([380, 380], interpolation=InterpolationMode.BICUBIC)\n\nmodel = efficientnet_v2_m(weights = torchvision.models.EfficientNet_V2_M_Weights, num_classes = CLASS_COUNT, dropout = 0.5)\n\n# model = efficientnet_b4(weights = torchvision.models.EfficientNet_B4_Weights, num_classes = CLASS_COUNT, dropout = 0.5)\n    \nmodel.to(DEVICE)\n\nmodel = nn.DataParallel(model)\n\n# criterion = torchvision.ops.sigmoid_focal_loss\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr = LEARNING_RATE)\nlr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer)\n# lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer = optimizer, T_0 = 20, T_mult = 2, eta_min = 1e-5)\nscaler = torch.cuda.amp.GradScaler(enabled = True)\nbest_score = 0\n\nstart_epoch = 1","metadata":{"_uuid":"a2026514-9a79-4b62-9775-55fd108c6bfb","_cell_guid":"b75602ce-99fe-4d0c-b8a6-584070132d57","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-22T22:14:15.026943Z","iopub.execute_input":"2023-04-22T22:14:15.027571Z","iopub.status.idle":"2023-04-22T22:14:15.046696Z","shell.execute_reply.started":"2023-04-22T22:14:15.027522Z","shell.execute_reply":"2023-04-22T22:14:15.045256Z"},"trusted":true},"execution_count":37,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/2137081765.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# model = efficientnet_v2_m(weights = torchvision.models.EfficientNet_V2_M_Weights, num_classes = CLASS_COUNT, dropout = 0.5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mefficientnet_b4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEfficientNet_B4_Weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCLASS_COUNT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'CLASS_COUNT' is not defined"],"ename":"NameError","evalue":"name 'CLASS_COUNT' is not defined","output_type":"error"}]},{"cell_type":"code","source":"torch.backends.cudnn.benchmark = True\ntorch.autograd.set_detect_anomaly(False)\ntorch.autograd.profiler.emit_nvtx(False)\ntorch.autograd.profiler.profile(False)\ntorch.set_float32_matmul_precision = 'medium'\n\nrun = wandb.init(project = PROJECT_NAME)\n\nfor epoch in range(start_epoch, start_epoch + EPOCH_COUNT):\n    TrainEpoch(model, loaders['train'], epoch)\n    \n    print(\"\")\n    \n    loss, score = ValidEpoch(model, loaders['valid'], epoch)\n    \n    lr_scheduler.step(loss)\n    \n    print(\"\")\n    \n    run.log({\"Learning Rate\" : lr_scheduler._last_lr[0]})\n    \n    if(score > best_score):\n        print(\"Found a Better F1 Score (%.6f). Saving the Model...\" % (score))\n        \n        best_score = score\n        \n        checkpoint = {\n            'epoch' : epoch,\n            'model' : model,\n            'criterion' : criterion,\n            'optimizer': optimizer,\n            'lr_scheduler' : lr_scheduler,\n            'scaler' : scaler,\n            'best_score' : best_score\n        }\n        \n        path = \"saved-model-%d | %.6f\" % (epoch, score) + \".pth\"\n        \n        torch.save(checkpoint, path)\n        wandb.save(path)\n        \n        print(\"\")\n        \n    print(\"\")","metadata":{"_uuid":"e20d9fec-dcb0-44e3-adb1-fe375712be34","_cell_guid":"5584ebf0-dfff-4d1b-b18f-0ca91f2dd037","execution":{"iopub.status.busy":"2023-04-16T20:55:08.894185Z","iopub.execute_input":"2023-04-16T20:55:08.894569Z","iopub.status.idle":"2023-04-16T20:55:08.934607Z","shell.execute_reply.started":"2023-04-16T20:55:08.894536Z","shell.execute_reply":"2023-04-16T20:55:08.932911Z"},"trusted":true},"execution_count":null,"outputs":[]}]}